---
title: Handwriting Styler, 내 손글씨를 따라쓰는 인공지능 (1) – 이론
views: 8417
date: "2019-07-20"
tags: ["GAN", "딥러닝", "생성모델"]
description: 사람의 손글씨에 담긴 스타일을 학습해 그 스타일대로 글씨를 생성하는 모델 만들기 첫 번째 여정
---

본 글은 개인 프로젝트로 진행된 프로젝트에 대한 소개글입니다.
[1편](https://jeina.io/post/handwriting-styler-1)에서는 **프로젝트의 큰 그림과 이론적인 내용**을,
[2편](https://jeina.io/post/handwriting-styler-2)에서는 **프로젝트의 실제 진행 과정에 대한 내용**을 소개합니다.
관련 코드는 [이곳](https://github.com/jeina7/Handwriting_styler)에 있습니다.

> 이 프로젝트는 중국어 글자를 이용해 진행된 [zi2zi](https://kaonashi-tyc.github.io/2017/04/06/zi2zi.html) 프로젝트의 많은 도움을 받았습니다.

<ThreeDots />

# 내 손글씨체를 기계가 학습할 수 있을까?

한글은 세계적으로 완성도가 높다고 평가되는 언어 중 하나입니다. 하지만 그렇게 우수한 한글의 유일한 단점은 복잡하다는거죠.
실생활에 사용되는 상용한글은 2,350자이고 자모음의 조합으로 만들어낼 수 있는 모든 글자는 총 11,172자에 이를만큼 어마어마한
다양성을 갖고 있기 때문입니다. 대소문자를 모두 합쳐도 52자밖에 안 되는 영어에 비하면 엄청난 양이죠.

이러한 한글의 특성 때문에 한글폰트를 만드는 일은 많은 시간과 비용이 드는 전문적인 작업으로 알려져 있습니다.
전문 디자이너가 작업한다고 해도 완전한 한글 폰트를 만들어내는 데에는 몇 개월 이상의 시간이 걸린다고 합니다.

그렇다면 **인공지능이 내 손글씨를 학습해서 만들어낼 수 있다면 어떨까요?** 사람이 몇 글자만 직접 써서 모델에게
넣으면 모델이 **글씨체의 특징을 파악해서 나머지 글자를 모두 만드는 거죠.** 이런 호기심에서 본 프로젝트가 시작되었습니다.

더 자세한 내용을 알아보기에 앞서 모델이 만들어낸 글자를 먼저 확인해보세요! 어느 쪽이 가짜일까요?

<Image src="result-1.png" height={329} />

꽤나 비슷하죠? 😉

각 열에서 오른쪽은 컴퓨터 폰트 그대로의 진짜 이미지이고 **왼쪽이 모델이 생성한 가짜 이미지**입니다.
언뜻 보기에는 같은 글자들처럼 보이지만 사실 자세히 보면 디테일들이 조금은 흐립니다. 그렇다면 손글씨같은 모양의 글씨체들은 어떻게 썼을까요?

<Image src="result-2.png" height={217} />

위의 곧은 글씨체들보다는 조금 더 어려워하는 글자들이 있는게 보이죠. 그래도 이만하면 꽤 잘 따라쓰는 듯 합니다.

# 1. 프로젝트의 큰 그림

사실 말은 거창하게 했지만 **글씨의 특징을 잘 학습해서 비슷한 스타일의 다른 글자들을 생성해낼 수 있도록 모델을 학습시키는 것**이
이 프로젝트의 기본적인 목표입니다. 이 목표를 달성하기 위해서 프로젝트는 크게 다음과 같은 두 가지 스텝으로 나뉩니다.

## 1) Pre Training, 사전 학습 모델 만들기

모델이 사람의 손글씨를 학습하기 전에 해야 할 일은 글자 이미지 자체를 학습하는 것입니다.
아주 많은 글자 이미지들을 이용해 글자의 특징을 추출하고 다시 복원하는 방법을 먼저 학습해야 하죠.
따라서 Pre Training 단계에서는 사람의 손글씨 대신 대량의 데이터셋을 만들기 쉬운 컴퓨터 폰트 이미지로 학습합니다.

<Image src="pre-training-dataset.png" w="w-88" width={352} height={92} />

모델은 위 그림과 같이 다양한 스타일의 25가지 폰트 당 랜덤으로 추출된 약 3,000장씩, 총 75,000여 장의
글자 이미지 (128 × 128)로 학습하였습니다.

Pre-Training 단계의 최종적인 목표는 다음 그림과 같습니다. 고딕체의 글자와 변환하기를 원하는
타겟 폰트 카테고리 정보를 함께 입력받으면 타겟 폰트가 입혀진 글자를 출력하는거죠.
즉, 입력은 고딕체 글자를 받지만 모델은 폰트가 입혀진 글자로 **스타일 변환**을 하도록 학습됩니다.

<Image src="pre-training.png" w="w-88" width={352} height={86} />

이렇게 75,000자의 데이터로 제대로 학습이 된다면 모델은 이제 컴퓨터 폰트 글자를 잘 쓸 수 있게 됩니다.
이 모델은 다음단계에서 사람의 손글씨를 학습할 모델의 베이스 모델이 됩니다.

## 2) Transfer Learning, 사람의 손글씨 학습하기

잘 학습된 사전학습모델이 준비되었다면 **Transfer Learning (전이 학습)**을 이용해 모델에게 사람의 손글씨를 학습시킵니다.

Transfer Learning이란 **이미 학습이 되어있는 모델을 다른 데이터셋이나 다른 목적의 프로젝트에 입맛에 맞게 적용시켜 사용하는 것**입니다.
사전에 학습되어 있는 모델을 사용하면 처음부터 학습시키지 않아도 되므로 시간과 자원을 절약할 수 있어 굉장히 유용하죠.

> Transfer Learning에 대해 더 자세히 알고싶다면 [이전에 번역한 포스트](https://jeina.io/post/transfer-learning)를 참고하세요.

첫 번째 단계에서 직접 학습시켜놓은 사전학습 모델이 있으니 충분히 활용해야겠죠.
이번 단계의 목표는 컴퓨터 폰트에 대해서는 거의 99% 완벽하게 베껴내는 사전학습 모델을 이용해
보다 어려운 **사람의 손글씨를 학습시키는 것**입니다. 따라서 이번 단계에서 모델이 학습하는 데이터셋은 사람의 손글씨입니다.

하지만 사람의 손글씨는 말 그대로 사람이 직접 써야하므로 컴퓨터 폰트만큼 대량으로 만들기 쉽지 않습니다.
또 우리의 목적은 폰트를 만들 때 사람이 직접 작업하는 양을 줄이는 것이므로 이번 단계에서 모델은 앞의 Pre Training 단계에
비해 **아주 적은 양의 데이터셋으로만 학습**합니다. 실제로 모델은 아래와 같이 템플릿 위에 직접 쓴 210자의 손글씨 이미지만 학습했습니다.

<Image src="handwriting.jpeg" height={351} />

이번 단계의 최종 목표는 적은 데이터만으로 사람의 글씨체의 특징을 효과적으로 학습해 나머지 11,172자도
비슷한 글씨체로 생성해내는 것이죠. 아래 그림은 제가 직접 쓴 손글씨와 모델이 생성한 글씨를 비교한 것입니다.

<Image src="compare.png" height={93} />

어떤가요, 꽤나 잘 따라쓰지 않나요? ☺️

지금까지 설명한 두 가지 학습 단계를 간단하게 정리하면 **1) 컴퓨터 폰트로 글씨를 먼저 학습하는 Pre Training 단계**와
**2) 사람의 손글씨를 학습하는 Transfer Learning 단계**, 두 가지입니다.

## 2. 모델의 구조

모델의 기본적으로 생성 모델인 **GAN (Generative Adversarial Networks)** 구조로 되어있습니다.
GAN이란 이미지 분류기 모델과는 달리 이미지를 직접 생성해내는 모델입니다.

> GAN에 대해 자세하게 알아보고 싶다면 [이 포스트](https://dreamgonfly.github.io/2018/03/17/gan-explained.html)를 추천드립니다.

가장 기본적인 GAN 구조를 그림으로 그리면 다음과 같습니다.

<Image src="original-gan.png" height={243} />

위와 같이 GAN은 파란색 부분인 **이미지를 생성해내는 Generator**와 빨간색 부분인
**진짜 이미지인지 가짜 이미지인지 검출해내는 Discrimitor**로 구성됩니다.

- **Discriminator**는 이미지를 입력받으면 진짜인지 가짜인지에 대한 예측 확률을 0~1 사이의 값으로 출력합니다.
  출력된 확률값으로 정답 레이블 `(True: 1, False: 0)` 과 비교해서 얼마나 잘 맞췄는지 피드백을 받으면서 점점 똑똑해집니다.
- 반면 **Generator**는 Discriminator 를 속이는 것이 목표입니다. 처음에는 제대로 된 이미지를 생성하지 못하던 Generator는
  Discriminator를 속일 수 있을만큼 점점 더 진짜같아 보이는 이미지 생성해내도록 학습합니다.

이렇게 Discriminator와 Generator가 서로 **겨루며 (Adversarial) 학습하는 것**이 GAN 학습의 기본 원리입니다.
GAN의 최종 목표는 **아무 의미가 없는 noise 벡터를 입력으로 받으면 의미가 있는 이미지를 생성해내는 것**이죠.

하지만 Handwriting Styler 모델의 목표를 다시 떠올려보면 어떤가요?

<Image src="style-transfer.png" height={175} />

모델의 입력 데이터는 고딕체의 글자이고 출력해야 하는 데이터는 스타일이 변환된 새로운 글자입니다.
즉, 입력 데이터가 noise 벡터인 GAN과는 비슷하면서도 명확하게 다릅니다.

따라서 이런 상황에서 모델이 가져야 하는 구조는 다음과 같습니다.

<Image src="model-structure.png" height={216} />

이렇게 Generator 부분은 이미지를 **낮은 차원의 벡터로 Mapping 시키는 Encoder**와 **다시 이미지로 복원하는 Decoder**
두 가지로 구성됩니다. Decoder 구조만 있는 GAN과는 Generator의 구조가 사뭇 다르죠.

모델의 Generator 구조와 기능에 대해 조금 더 자세히 살펴보면 Generator는 앞서 언급했듯 고딕체를 새로운
글씨체로 바꾸는, 일명 **스타일 변환 (Style Transfer)**을 수행해야 합니다. 이를 위해서는 모델
변환하기를 원하는 폰트의 카테고리를 넣어 줄 필요가 있죠. 이 폰트 카테고리는 Category Vector 또는
Style Vector라는 이름으로 모델에 입력됩니다.

Encoder에는 다음 그림과 같이 z벡터와 카테고리 벡터가 합쳐진 벡터가 입력됩니다. 이런 파이프라인을 통해 어떤 카테고리
벡터를 넣느냐에 따라 내가 원하는 폰트로 변환을 할 수 있게 되죠.

<Image src="category-vector.png" height={216} />

여기서 카테고리 벡터를 Encoder에서 특징 추출이 끝난 뒤에 입력하는 것은 다음과 같은 의미를 가집니다.

- Encoder는 폰트와 관계없이 **고딕체 글자를 글자 종류에 따라서만 특징을 효과적으로 추출하는 데에만 집중**합니다.
  예를 들어 '맘'이라는 글자를 입력받으면 폰트를 고려할 필요 없이 '왼쪽 위에는 ㅁ이 있고, 오른쪽 위에는 ㅏ가,
  그리고 밑에는 ㅁ이 있다' 는 특징만을 잘 추출할 수 있도록 글자 특징 추출기로써의 역할에 충실하도록 학습하죠.
- 반면 Decoder는 **Encoder가 잘 추출한 z벡터와 함께 style vector를 입력받으면 글자를 각 style에 따라 복원하도록** 학습됩니다.

즉, 이렇게 Encoder와 Decoder의 역할을 분명히 구분함으로써 각자는 자기가 맡은 일을 더 잘 할 수 있게 하는 효과를 가지게 됩니다.

마지막으로 Generator의 구조를 3D 형태로 보면 다음과 같습니다.

<Image src="3d-structure.png" height={173} />

여기서 마지막으로 확인할 수 있는 건 Encoder에서 단계적으로 축소되어가는 벡터들을 모두 Decoder에 단계적으로 입력하는
**[U-Net](https://arxiv.org/abs/1505.04597)** 구조입니다.

UNet 구조를 개념적으로 설명하자면 이미지를 복원해야하는 Decoder에게 Encoder에서 추출되는 정보를 추가로 입력해줌으로써
약간의 도움을 주는 것이라 볼 수 있습니다. 실제로 UNet이 처음 발표된 이후 뛰어난 성능을 인정받았고
이후 GAN 구조에서도 많이 사용되었습니다.

# 3. Losses, 여러가지 손실함수

GAN은 [학습시키기가 까다롭다고 유명한](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)
모델입니다. Generator와 Discriminator가 서로 겨루며 학습한다는 아이디어는 좋아보이지만
실제로는 그 과정에서 여러 애로사항들이 있죠.

예를 들어 Discriminator가 똑똑해지는 속도는 매우 빠르지만 Generator가 똑똑해지는 데에는 오랜 시간이 걸리기 때문에
학습 속도면에서 차이가 있습니다. 위조지폐를 검출해내는 것이 빠를지, 아니면 걸리지 않을 위조지폐를 밑바닥부터 만드는 것이 빠를지
생각해보면 간단하죠.

또한 Generator는 Discriminator를 속이는 것만이 목표이기 때문에 다양한 이미지들을 만들기보다는
안 걸리는 하나의 이미지를 찾으면 그걸 더 잘 만드는 쪽으로 학습이 될 수도 있습니다. 이 또한 생각해보면 꽤나 논리적입니다.
Generator 입장에서는 어렵게 다양한 이미지들을 만들면서 Discriminator를 속이려고 노력하는 것보다
운 좋게 Discriminator가 속아넘어가는 이미지 하나를 잘 만들었다면 그 이미지만 쭉 대량 생산하는거죠.
이런 문제를 **Mode Collapse**라고 합니다.

> Mode Collapse에 대한 자세한 내용은 [이곳](http://jaejunyoo.blogspot.com/2017/02/unrolled-generative-adversarial-network-1.html)을 참고하세요.

이렇듯 여러가지의 불안정성 문제로 GAN을 잘 수렴시키기 위해서는 잘 설계된 손실함수가 필요합니다. Handwriting Styler 모델 학습에는
다양한 손실함수가 사용되었는데 Generator의 손실함수와 Discriminator의 손실함수를 구분해서 확인해 보겠습니다.

## Loss of Discriminator

Discriminator의 목표는 간단합니다. 이미지를 입력받으면 진짜 이미지인지 가짜 이미지인지 판단하는 동시에,
이미지의 폰트 카테고리까지 예측하는거죠. 이에 따른 손실함수는 다음과 같습니다.

### 1) Binary Loss

입력받은 이미지가 Generator가 생성한 가짜 이미지인지 혹은 진짜 이미지인지 구분하는 Loss입니다.
따라서 진짜 이미지일 예측 확률을 0~1 사이의 값으로 출력하고 그 값을 정답 레이블 `(True: 1, False: 0)` 과 비교합니다.
loss는 이진 분류기에 많이 쓰이는 **Binary Cross Entropy**로 계산합니다.

### 2) Category Loss

입력받은 글자의 폰트 카테고리를 예측하는 loss입니다. 예를 들어 입력받은 이미지가 1번 폰트인지, 2번인지 또는 10번인지
Multi Class Category를 예측하고 각 폰트 카테고리에 대한 예측값을 역시 0~1 사이의 확률값으로 출력합니다.
이는 Generator가 제대로 된 폰트 스타일을 생성할 수 있도록 도움을 줍니다. loss는 역시 위와 같은 **Cross Entropy**로 계산합니다.

## Loss of Generator

Generator는 진짜같아 보이는 이미지를 생성하도록 하는 방향으로 학습해야 합니다. 이를 위한 loss는 다음과 같습니다.

### 1) L1 Loss

Generator의 가장 기본 목표를 위한 loss 입니다. Generator가 이미지를 생성하면 생성된 이미지 (Fake Image)를 타겟으로 하는
진짜 이미지 (Target Image) 와 비교해서 loss를 계산합니다.

<Image src="l1-loss.png" w="w-60" width={240} height={128} />

loss는 두 이미지를 pixel by pixel로 비교하는
Mean Absolute Error(MAE)로 계산됩니다. MAE loss가 크면 두 이미지가 많이 다르다는 의미이므로
Generator는 두 이미지가 비슷해지도록 생성하는 방향으로 학습됩니다.

### 2) Constant Loss

[DTN Network](https://arxiv.org/abs/1611.02200) 논문에서 처음 소개된 Constant Loss는
생성된 이미지 (Fake Image)와 고딕체의 원래 이미지 (Source Image)를 각각 Encoder에 통과시킨 후 만들어지는
z벡터간의 차이를 계산한 loss입니다.

<Image src="constant-loss.png" w="w-60" width={240} height={201} />

Encoder는 이미지의 특징을 추출하므로 Fake Image는 Source Image와 비슷한 위치에 매핑되어야 원래 글자의
형태를 잃어버리지 않을 것입니다. 따라서 Constant loss는 두 z벡터가 비슷하게 유지되도록 제어하는 역할을 합니다.

### 3) Category Loss

Generator 또한 Discrinimator에 생성한 이미지를 입력해 출력되는 폰트의 카테고리에 대한 점수로 피드백을 받습니다.
Generator는 1번 폰트의 글자라고 생성했는데 Discriminator가 10번 폰트의 글자라고 판별하면 잘 만들지 못한거겠죠.
이렇게 Generator는 Discriminator가 올바른 폰트 카테고리로 분류할 수 있을만큼 폰트의 특징이 잘 반영된 글자를 만들 수 있도록 학습됩니다.

여기까지 Handwriting Styler 프로젝트의 큰 그림 모델의 구조, 그리고 손실함수 등 이론적인 내용들을 담아보았습니다.

이 이후로는 본격적으로 프로젝트의 생생한 진짜 진행 과정을 살펴볼 예정입니다.
데이터 전처리부터 모델을 학습시키는 Hyper Parameter까지 세부적인 내용들은 [다음편](https://jeina.io/post/handwriting-styler-2)에서 확인하실 수 있습니다.

<ThreeDots />

## Acknowledges

- [zi2zi: Master Chinese Calligraphy with Conditional Adversarial Networks](https://kaonashi-tyc.github.io/2017/04/06/zi2zi.html) by [kaonashi-tyc](https://github.com/kaonashi-tyc)
- [zi2zi: GitHub Repository](https://github.com/kaonashi-tyc/zi2zi) by [kaonashi-tyc](https://github.com/kaonashi-tyc)
- [AC-GAN](https://arxiv.org/abs/1610.09585)
- [DTN Network](https://arxiv.org/abs/1611.02200)
- [pix2pix-tensorflow](https://github.com/yenchenlin/pix2pix-tensorflow) by [yenchenlin](https://github.com/yenchenlin)
- [Domain Transfer Network](https://github.com/yunjey/domain-transfer-network) by [yunjey](https://github.com/yunjey)
- [ac-gan](https://github.com/buriburisuri/ac-gan) by [buriburisuri](https://github.com/buriburisuri)
- [dc-gan](https://github.com/carpedm20/DCGAN-tensorflow) by [carpedm20](https://github.com/carpedm20)
- [origianl pix2pix torch code](https://github.com/phillipi/pix2pix) by [phillipi](https://github.com/phillipi)
