---
title: AI x Bookathon｜인공지능을 수필 쓰는 작가로 학습시켜보자
views: 9725
date: "2019-11-25"
tags: ["GPT", "딥러닝", "생성모델"]
description: koGPT-2에서 수필 쓰는 생성 모델로 업그레이드 하기. 크롤링부터 모델 준비, 그리고 학습까지
---

# 인공지능과 함께 글쓰기 – AI x Bookathon in SKKU

지난 11월 21-22일, 성균관대학교에서 무박 2일동안 진행되는 [AI x Bookathon](http://www.donga.com/news/article/all/20191124/98510802/1)
대회를 열었습니다. 본 대회는 학교에서도 처음으로 열게 된 대회였는데요, 인공지능을 개발하는 기업
[마인즈랩](https://www.veritas-a.com/news/articleView.html?idxno=300782)과 협약을 맺고 주최했다고 합니다.

대회 내용은 지난 2월 OpenAI에서 발표한 [GPT-2](https://openai.com/blog/better-language-models/) 모델을 학습시켜서
하나의 수필 작품을 완성해내는 것이었습니다. GPT-2 원래 모델은 당연히 영어 기반으로 학습된 모델이었죠. 이 모델 구조를 그대로 가져와
한글 뉴스기사 약 40GB로 학습시킨 사전학습 모델과 거대한 모델을 학습시키는 데에 필요한 Nvidia의 V100 GPU를 마인즈랩
측에서 제공해주었습니다.

대회에는 다양한 전공 배경의 학생들이 참가했습니다. 학부생 및 대학원생까지 총 11팀이 참가해 약 25시간동안 쉬지 않고 열심히 달린 대회였어요. 💪🏼

저는 두 명의 친구들과 함께 팀을 꾸려 대회를 참가했습니다. 그 결과.. **대상**이라는 큰 상을 받아 왔어요..🎊🎉 기술적으로 많이 고민하고
접근했던 다른 팀들도 있었기에 그 중에서 1등을 할 수 있을까 싶은 마음이 있었는데 생각했던 것보다 좋은 상을 받게 되어서 얼떨떨하기도 하고..
너무 좋기도 하고..

그래서 대회를 어떻게 진행했는지, 그리고 어떤 부분이 강점으로 어필되었다고 생각하는지 등의 부분에 중점을 두고 정리해보려고 합니다.
대회의 마지막에 프레젠테이션 했던 내용을 중심으로 전체적인 대회 진행 과정을 풀어서 이야기해보도록 하겠습니다.

> 대회에서 발표했던 발표자료는 [이 곳](https://www.slideshare.net/MinjungChung1/ai-bookathon-public) 에서 보실 수 있습니다.

<ThreeDots />

# 1. 프로젝트의 시작, 모델에 대한 이해와 전략

대회에 대한 전체 내용은 대회로부터 약 일주일 전에 있었던 사전 교육에서 처음 들었습니다. 그 전에는 인공지능을 이용해서 글을 쓰는 대회를
한다고 한다.. 정도의 러프한 내용만 알고서 대회 참가 신청을 했기 때문에 자세하게 무슨 모델을 사용하는지, 대회의 구체적인 목표는
무엇인지 등을 알지 못했어요. 사전교육에서 처음으로 GPT-2 모델을 활용할 것이라는 내용을 전달받았고 그와 함께 GPT-2 모델, 그리고
모델을 training 또는 inference 하는 몇 가지 코드를 받게 되었습니다.

저희 팀은 개발을 공부하는 친구 둘과 딥러닝을 공부하는 저까지 셋으로 구성이 되어있었는데 저 또한 이미지 생성 모델을 다뤄본 경험은
있지만 자연어 모델을 직접적으로 다루는 것은 처음이었습니다. 그렇기에 프로젝트의 첫 시작은 GPT-2 모델에 대해 공부하는 것부터였습니다.
팀원 모두와 함께 GPT에 대해 자료조사를 해보고 모델을 어떻게 활용할 수 있을지에 대해 공부하기 시작했어요.

모델에 대한 이론적이고 수식적인 모든 내용들을 충분히 숙지할 수 있다면 더 좋았겠지만 대회의 성격이 무엇보다 오랫동안 진행되는 것이
아니라 일주일이 채 안되는 사전 준비 시간과 함께 무박 이일로 짧고 굵게 진행되는 것이었기 때문에 저희도 빠르게 훑으며 이해하는
방식을 취했습니다.

특히 대회 전에는 다른 것보다 제공받은 코드를 최대한 이해하는 것에 가장 많은 시간을 썼던 것 같습니다. 아무리 이론에 대한 지식이
많다고 해도 제공받은 코드, 그리고 대회에서 직접 사용할 코드에 대한 이해도가 부족하다면 대회 당일에 많은 브레이크가 걸릴테니까요.

그래서 코드의 구조를 파악하면서 개별 코드들까지 모두 익히는 데에 대회 준비 시간을 거의 썼는데요, 이 중 가장 까다로우면서도
새롭게 배우게 된 것 중 하나는 데이터가 저장된 포맷인 `hdf5` 형식이었습니다.

`hdf5` 파일 형식은 대규모의 데이터를 효과적으로 저장 및 사용할 수 있는 포맷입니다. 저는 이번 대회를 진행하면서 처음 접했는데
사용법을 익히는데 많은 시간이 들었습니다. 만약 데이터까지 다루면서 샘플 코드를 사용해보지 않았더라면 이 데이터 형식을 사용한다는
것을 몰랐을 것이고, 그 상태로 대회에 들어갔다면 다른 결과가 나왔을 수도 있었을 듯 합니다.

처음에는 `hdf5` 파일이 판다스와 쉽게 호환이 될 거라고 생각해서 판다스 포맷으로 변환해서 사용하려고 했는데 잘 안돼서 조금 더
알아보니 데이터를 폴더 형식으로 하위 폴더 / 상위 폴더 개념으로 정리해두는 형식이더라구요.

> hdf5에서는 이 폴더 개념을 group이라고 합니다.

파이썬에서 `hdf5` 파일을 다룰 때는 dictionary 자료형처럼 key로 접근을 하는데 이 key가 바로 폴더의 이름과 같은 역할을 합니다.
파이썬에서는 `h5py`라는 패키지를 활용해 `hdf5` 파일을 저장하거나 사용할 수 있습니다.

> hdf5 파일에 대한 더 자세한 개념과 코드는 [이 곳](https://hiseon.me/python/h5py-hdf5/)을 참고하세요.

이렇게 모델과 데이터, 그리고 코드를 이해하면서 대회의 큰 골자를 이해해보자면 다음과 같습니다.

<Image src="todo.png" w="w-88" width={325} height={261} />

구조를 살펴보면.. 우연히도 이전에 제가 진행했던 [내 손글씨를 따라쓰는 인공지능](https://jeina.io/post/handwriting-styler-1)
프로젝트와 큰 줄기가 많이 닮아있습니다. 대용량 데이터로 학습된 사전학습모델을 이용해 적은 양의 데이터로 Transfer Learning 시킨다는 점이
비슷하죠.

이전에 프로젝트를 진행하며 배운 것을 토대로 이 프로젝트의 큰 줄기를 이해하고나서는 아, 엄청난 대용량의 데이터로 학습된 모델은
이미 제공받았으니 Transfer Learning 과정을 잘 진행하는 것이 핵심일 것이고 그 과정에는 **소량의 데이터만으로도 학습을 시킬 수
있겠다**는 아이디어가 떠올랐습니다. 이전에 진행했던 손글씨 프로젝트의 목표가 Transfer Learning에서 소량의 손글씨 데이터만으로
학습시키는 것이었기 때문이죠. 그래서 대회의 전략을 소량의 데이터를 사용하되 데이터의 품질을 올리는 데에 중점을 두는 것으로 설정했습니다.

한 가지 비유를 덧붙여보자면 Backbone이라고도 하는 사전학습모델은 마치 정교하게 잘 쌓여진 건축물과 같다고 할 수 있습니다.
많은 연구와 실험, 그리고 실패를 통해 그 구조의 유효성이 검증된 모델이죠. 이미지를 다루는 CNN 계열에서 가장 많이 쓰이는 백본 모델의 예로는
VGG, ResNet 등이 있습니다.

이러한 백본 모델을 **정교하게 잘 짜여져 있지만 아직 색이 안입혀진 무채색의 건축물**이라고 한다면
Transfer Learning은 **그 건축물 위에 원하는 색을 입히는 작업**이라고 할 수 있을 것 같습니다. 대용량의 데이터로 가중치들이
정교하게 학습이 되어있는 Pre-Trained 모델이 있다면 거기에서 Fine Tuning – 말 그대로 미세한 튜닝을 통해 원하는 방향으로 살짝만
틀어보는 것이죠. 이번 대회에서는 뉴스기사 문체에서 수필 문체로 가야 하겠죠.

따라서 이미 잘 학습된 백본 모델의 성능을 더 올린다거나 그 구조를 바꿔보는 것은 그만큼 많은 시간과 노력이 필요합니다.
하지만 이번 대회는 앞서 설명했듯 긴 시간이 주어지면서 기술적 또는 이론적으로 많은 연구를 진행해야 하는 성격의 대회가 아니었기 때문에
**이미 잘 학습된 백본 모델은 최소한으로 다루면서 Transfer Learning 단계에서 최대한으로 결과물의 품질을 끌어올린 것**이
이번 대회에서 좋은 성적을 거둘 수 있었던 가장 큰 포인트였다고 생각합니다.

실제로 마지막 프레젠테이션 때 심사위원이셨던 교수님께서 "이 팀은 사전 학습 모델에 대해서는 기술적으로 다룬 부분이 없는 것인가?"
라고 질문을 주셨는데 제가 "이번 대회는 짧은 시간 내에 정해진 포맷의 완성작을 제출해야 했기 때문에 사전학습모델을 건드리기보다는,
데이터의 품질을 최대한 끌어올리며 작품의 퀄리티를 높이는 것을 가장 중점으로 두었습니다." 라고 솔직히 대답했고, 대답을 들으신 후에는
이 부분이 전략적으로 잘 접근한 것이라 할 수 있다고 말해주셨습니다.

# 2. 데이터 품질을 끌어올려보자 – 데이터 수집, 그리고 전처리

데이터의 품질을 최대한으로 끌어올린다는 전략 하에 저희의 데이터 수집은 웹사이트 크롤링으로 진행되었습니다. 먼저 데이터를 수집할
사이트를 검색해서 리스트업을 한 뒤 선정된 사이트에 대해 파이썬의 `scrapy`와 `selenium` 라이브러리로 크롤링을 돌려서 수집이 되는지 확인해보는
것까지 진행을 하고 대회에 들어갔습니다. `hdf5` 포맷을 다루는 방법을 익히는 데에 시간이 많이 걸려서 대회 시작 직전에야 `txt` 형식의
데이터를 `hdf5` 형식으로 변환하는 것을 성공했고 대회 시작을 하고 나서 처음으로 저희가 수집한 데이터로 직접 학습을 시켜보았습니다.

> 전체 코드는 [이 곳](https://github.com/jeina7/GPT2-essay-writer/tree/main/notebooks) 에서 확인하실 수 있습니다.

크롤링을 하면서 주안점을 두었던 부분은 이전에도 계속 언급하였듯 데이터의 양보다 **품질**이었습니다.
우리가 최종적으로 생성하고 싶은 수필 성격을 띄는 데이터를 수집하는 것에 신경을 많이 썼고,
다량의 데이터를 수집하면서 원하지 않는 데이터가 최대한 들어오지 않도록 노력하였습니다.

특히 글 목록 중 공지글이나 '필독' 등의 수필이 아닌 글이 많이 있었기에 이 부분은 일괄적으로 제거했고,
점과 반점 등의 기본적인 기호를 제외한 특수문자는 모두 제거하는 간단한 전처리를 진행하였습니다.

데이터 수집 초기에는 그래도 데이터가 조금 더 있으면 좋지 않을까 하는 생각에 수필과 소설 데이터를 최대한 끌어모아서
약 70MB의 데이터를 수집했지만 소설 데이터에는 대화체가 너무 많거나 비교적 가벼운 문체가 있는 경우가 많았습니다.
그래서 수집한 모든 소설 또는 원하지 않는 스타일의 수필 등을 전부 직접 눈으로 확인하며 데이터를 전체적으로 정제하는 작업을 거쳤습니다.

모든 데이터 정제를 거친 뒤 최종적으로 약 5MB의 적지만 질 좋은 데이터를 모아 최종 학습 데이터로 선정하였습니다.
수집된 텍스트 데이터는 모델과 함께 제공받았던 `tokenizer` 객체를 이용해서 토크나이징을 한 후 `hdf5` 파일로 변환해 저장하였습니다.

텍스트 데이터의 토크나이징이란 일정 단위로 사람이 쓰는 자연어를 쪼개는 것을 말합니다. 대회에서는 Mecab 기반의 tokenizer를
사용했는데 데이터 전처리를 진행하면 다음과 같이 형태소 단위로 쪼개지게 됩니다.

<Image src="mecab.png" height={205} />

'나는' 이라는 짧은 단어도 '나'와 '##는' 이라는 두 형태소로 쪼개지는데, 여기서 ## 표기는 해당 #이 표시된 위치에 다양한 단어가
올 수 있다는 것을 뜻합니다. 한국어에서는 주로 형용사나 조사 또는 접미사 등에 붙게 됩니다.

이렇게 토크나이징까지 모두 거친 후 각 토큰에 대한 id를 부여해 숫자 데이터로 변환하고 나면 모델에 입력될 수 있는 `array`
형태로의 변환이 끝나게 됩니다. 여기까지 전처리를 진행한 한 후 `hdf5` 파일 형식으로 모델에 바로 입력할 수 있도록 저장하였습니다.

# 3. 그럼에도 불구하고, 더 좋은 학습을 위해

지금까지 잘 정제되고 전처리 된 데이터를 준비했는데요, 데이터의 품질은 실제로 결과물의 품질에 얼마나 영향을 미칠까요? 앞서 말씀드렸듯
사전 학습 모델은 이미 정교하게 잘 학습되어 있는 모델입니다.

> 물론 그렇다고 모든 사전 학습 모델이 완벽하다는 것은 아닙니다. 실제로 Transfer Learning의 최종 결과물은 사전 학습 모델의
> 성능에 가장 큰 영향을 받습니다. 즉, 할 수 있다면 사전학습모델의 성능을 최대한 끌어올리는 것이 결과물 품질을 올리는 데에 가장
> 효과적이지만 현실적으로 이 방법이 어렵다면 – 사전학습모델에 대한 연구가 이미 많이 이루어져서 어떻게 성능을 올려야 할지
> 모르겠다거나 그에 대한 연구를 할 시간이 없다면 – Transfer Learning을 하는 데이터의 품질을 올리는 것이 그 다음의 차선책이라는 뜻입니다.

지금 이 프로젝트에서는 시간상의 한계로 Transfer Learning의 데이터 품질을 끌어올리는 전략을 취했었습니다. 그리고 데이터의 양 또한
비교적 적어도 잘 학습이 될 것이라고 기대했죠! 그렇다면 실제로 데이터의 품질이 학습 결과물의 품질에 영향을 준 것이 맞을까요?
이것에 대한 확인을 먼저 진행해보도록 하겠습니다. 정제되지 않은 데이터로 학습시켜본 결과를 먼저 확인해보시죠.

GPT 모델은 기본적으로 어떠한 context를 입력받으면 그 뒤에 이어질 문장을 생성해내는 모델입니다. 따라서 먼저 대회에서 제공받은
대량의 뉴스기사로 학습된 기본 사전학습 모델과 정제되지 않은 데이터로 5000step을 학습한 모델, 두 가지 모델에 같은 문장을 입력하면
그에 대한 결과물이 어떻게 다르게 나오는지 각각의 결과물을 비교해보겠습니다.

모델에 입력한 context는 다음과 같습니다.

> 나는 어려서부터 노래를 좋아했다. 설거지를 하거나 청소를 하며 흥얼흥얼 노래를 하면 시간이 빨리 간다.

1. **뉴스기사로 Pre Trained된 모델의 결과**  
   '...한다' 혹은 '...하다' 와 같은 기사의 문체가 남아있는 것을 확인할 수 있습니다.

<Image src="pretrained-result.png" height={60} />

2. **정제 안 된 데이터로 5000step을 학습한 모델**  
   짧게 학습했을 뿐인데 문체가 바로 수필과 비슷한 문체로 바뀌었습니다.

<Image src="finetuned-result.png" height={40} />

둘을 비교해보면 적은 데이터로 5000step 정도만 학습해도 바로 수필의 문체가 나오는 것을 확인할 수 있습니다.
사전학습모델이 약 4000만 step을 학습한 모델이라는 점을 생각하면 놀라운 결과입니다. 하지만 아직 기뻐하긴 이릅니다.
어느정도 문체가 바뀐 것 같기는 하지만 아직까지는 부족한 부분이 많거든요.

결과물 샘플을 조금 더 찍어보면 다음과 같은 결과물들이 나오기도 합니다.

<Image src="not-good-result.png" height={274} />

아직까지는 안정적인 결과물을 내지 못하는 모습이죠. 이러한 한계점들이 있은 후 저희 팀은 결과물 품질을 올리기 위해서는
데이터의 품질을 더 좋게 만들어야 한다고 생각하였습니다. 그래서 소설 데이터를 제거하고 눈으로 직접 모든 데이터들을 확인하며
원하는 품질의 데이터로 깔끔하게 정제했죠.

그렇게 심혈을 기울여 정제한 데이터로 학습시킨 모델에 위에서와 같은 문구를 입력했을 때 나오는 최종 결과물은 다음과 같습니다.

1. **데이터를 정제한 뒤 35,000step 학습시킨 모델**

<Image src="better-result.png" height={29} />

위에서 봤던 불안정한 모델의 결과보다 훨씬 말이 되는 결과물을 내놓습니다. **'평범한 일상생활을 하는 내게 있어 노래하는 시간은
보너스처럼 느껴질 때다.'** 정도라면 꽤나 괜찮은걸요.

잘 생성된 샘플 중 인상깊었던 것을 하나 더 가져와보겠습니다.

<Image src="better-result-2.png" height={197} />

개인적으로 가장 마음에 드는 문당은 **"깨닫게 하는 것이 깨달음이다. 진리라는 것은 어디에건 다 있다."** 부분입니다. 😂

더 재미있는 건, 이건 학습 중간중간 학습 상태를 확인하기 위해 랜덤으로 생성하던 샘플 중 하나라서 입력값이 한 문장이 아니라
맨 앞의 **"그러한"** 세 글자 뿐이었다는 사실입니다. 이 모델은 `그러한` → `깨달음` → `속에서` → `나는` → `스님이` ... 이런 식으로
한 단어씩 바로 앞 문맥과 어울리는 단어를 뱉어내며 위의 문단을 완성해내었습니다.

마지막으로 학습과정에 있었던 개선점을 한 가지 더 보여드리고 마무리하도록 하겠습니다! 바로 학습을 하던 중 갑자기 발산해버린 상황이
있었는데요, 모델이 학습을 잘 하다가 다음과 같이 **\[PAD\]** (공백) 만 내뿜으며 발산해버렸습니다.

<Image src="pad.png" height={196} />

이런 경우에는 보통 learning rate를 많이 떠올리게 됩니다. 저번 손글씨 프로젝트를 진행했을 때도 어느 순간부터 학습이 안되며
매번 같은 이미지만 생성했던 상황이 있었습니다. 따라서 이 상황에 대해서는 learning rate를 학습 step 단계별로 순차적으로
decay(감쇠)를 해줌으로써 해결할 수 있었습니다.

<Image src="decay.png" w="w-88" width={352} height={205} />

# 4. 프로젝트의 끝, 우리가 아닌 AI가 쓴 글

지금까지 많은 과정을 거쳐 저희가 진행했던 프로젝트에 대해 이야기해 보았습니다. 그럼 이제 드디어 저희의 완성작을 한 번 확인해보겠습니다.
저희가 작품을 완성시킨 방식은 **최대한 AI 모델이 주도적으로 작성하도록 하는 것**이었습니다. 프로 휴먼 수필러가 작성한만큼의 완성도는
안되더라도 노력하는 신인작가 정도의 레벨이 되는 우리 모델이 직접 처음부터 끝까지 완성할 수 있도록요! 😂

대회의 주제어는 바로 **"만약 - IF"** 이었습니다. 그래서! 저희는 과감하게 맨 처음에 모델에 "만약" 두 글자를 입력했습니다.
그렇게 뱉어낸 문장들 중 마지막 문장을 다시 문장에 넣고, 또 마지막 문장을 넣으면서 전체 글을 완성해나갔습니다.

즉, 저희는 모델을 팔로잉만 하고 글의 모든 소재와 흐름은 모델이 결정하도록 한 것이죠. 실제로 작품의 완성까지 그렇게 진행을
했고 마지막에 저희가 수정한 내역은 불필요한 부분을 삭제하기 또는 오탈자 또는 간단한 문법적인 부분을 수정하기 등 퇴고를
하는 느낌으로 진행하였습니다. 결국 전체적인 글은 모두 모델이 작성하고 저희는 살짝 잡아주는 역할만 한 것이죠.
사실 글을 자세하게 보면 어순 또는 서술관계가 조금 어색한 문장들이 있기도 하지만 AI가 온전히 작성한 부분들을 최대한 남기기 위해
더 많이 건드리지 않은 부분들이 있기도 합니다.

그래서 그렇게 완성된 결과물은 어떻게 나왔을까요? 저희도 예상치 못한 스토리가 전개되었습니다. 글이 완성되고 보니 글의 주제는
**인생의 끝자락에서 과거를 회상하며 여러가지 만약을 떠올리는 노인의 회고록**이 되었더라구요. 전체적인 이야기의 흐름은 다음과 같습니다.

<Image src="story.png" w="w-88" width={352} height={310} />

위 내용 중 젊은 시절의 일기장이라던지 거울을 보며 작아진다는 내용, 그리고 돌아가신 아버지와 사별한 아내라는 소재나 철없던 젊은 시절
등은 모두 모델이 생성해낸 내용들입니다.

이 중 특히 잘 쓴 문장이라고 생각되는 문장들은 이런 것들이 있습니다. 문학적인 문장들이 굉장히 많아서 나름 만족스러운 결과물이었습니다.

- 오늘의 실패가 거울에 비치고 매번 똑같은 표정을 짓고 있는 모습에 나는 한없이 작아졌다.
- 허물어지고 보니 세월은 삼천을 지나온 것 같다. 세월은 예나 지금이나 흔적을 보이지 않는다.
- 궁핍할 때는 맛보기만 해도 배부르셨는데 지금은 흔치 않은, 깊은 산 자락에서 단 한 번의 해변도 가지 못하고 쓸쓸히 계셨던 아버지를 그려본다.
- 아버지에 대한 회상은 길게, 몇 초 만에 내 마음 깊은 곳의 이야기가 풀려나며, 마음속에 선명한 기억으로 아로새겨진 것처럼 내 감정도 완고해졌다.
- 실연당한 사랑의 아픔을 노래하던 그 노래가 몇 년 만에 다시 나에게 돌아왔다. 짧은 사유의 표현이라고 짐작하면 안 되지만, 그것도 생각보다는 오랜 기간 나를 짓누르고 있던 외로움이었을까.
- 속박되고 미워만 가는 나의 마음을 달래주었던 것은 사랑의 형형색색의 아름다운 색깔, 그 수많은 꽃들과 함께 이 세상에 여운을 남기며 가없는 사랑을 속삭였던 여인이었다.
- 모두 이 겨울 하늘을 보면서, 모두가 아, 무감각해졌다는 것을 알게 될 것이다. 그것은 마치 이 남은 시리운 과정을 거쳐 꺼져가는 불씨를 보듯이.
- 나는 고독이라는 것에, 그리고 기쁨을, 그리고 안락을 통해 모두 함께함으로써 위안해 주고 싶다. 슬픔도 고통도 행복도 나의 삶이니 어찌 싫어하랴.
- 사실 내가 그 정도의 콧대를 가지고 그다지 흥미와 호기심이 없던 내가 못다한 숙제를 얻은 기분이었다. 그러면서도 백일장이라니! 살짝 어깨가 들썩거려 걸음을 재촉했다.
- 그동안 잘해 왔는가 늘 마음에 걸리셨던 어머니. 나를 낳고 지금껏 살아오는 동안 얼마나 참담했을까. 어머니는 내게 모든 것을 딛고 일어서라고 말씀하셨다. 나는 끝내 눈물을 쏟아야 했다.
- 욕심없는 마음으로 번민의 보배목이 되어 나의 품위를 다듬고, 나의 등불같은 삶을 풍요롭게 가꾸면 되는대로 살아가면 되는 것이겠지.

> 원문은 [여기](https://github.com/jeina7/GPT2-essay-writer/blob/main/full_essay.md)에서 보실 수 있습니다.

<ThreeDots />

여기까지 저희의 무박 이일동안 달려왔던 전체 프로젝트 과정을 소개해 드렸습니다. 부족한 점도 있고 나름 잘 해낸 점도 있는 것 같습니다.
무엇보다 하나의 프로젝트를 완성했다는 점에서 또다른 좋은 경험이 되었어요.

이런 좋은 기회를 제공하고 대회기간 내내 우리 참가자들의 안녕을 위해 많은 신경을 써주셨던 학교와 관계자님들께 감사드리고, 또 장장
25시간동안 지치지 않고 달려줬던 우리 팀에게도 무한 박수를 보냅니다. 👏🏼👏🏼
